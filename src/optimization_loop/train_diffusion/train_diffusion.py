import numpy as np
import os , sys
sys.path.append("..")
sys.path.append("../..")
sys.path.append("../../..")
sys.path.append("src/")
sys.path.append("src/diffusion_core")
sys.path.append("src/diffusion_notebooks")
sys.path.append("data/")
sys.path.append(os.path.abspath(".."))
sys.path.append(os.path.abspath("../.."))
sys.path.append(os.path.abspath("../../.."))
from diffusion_core.diffusion import GaussianDiffusion1D
from diffusion_core.model import Unet1D
from diffusion_core.datasets import AirfoilDataset
from torch.utils.data import DataLoader
import torch
import os,sys
import torch
import os
from pathlib import Path
from surrogate_models.models import Hybrid_surrogate_MLP
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import matplotlib.pyplot as plt
from pathlib import Path
import torch.nn  as nn
import yaml
from torch.utils.data import Dataset, TensorDataset
import time
import warnings
from optimization_loop.Inner_loop.models import UA_surrogate_model
warnings.filterwarnings("ignore")

# --------------------------------------------------------------------

def update_lr(optimizer, lr):
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

def inv_coords(xs_s):                   # xs_s shape (...,2,192) tensor
    xs_np = xs_s.permute(0,2,1).cpu().numpy()    # -> (B,192,2)
    # xs_np[...,0] = xs_np[...,0]*(x_max-x_min) + x_min
    # xs_np[...,1] = xs_np[...,1]*(y_max-y_min) + y_min
    out = torch.from_numpy(xs_np).to(xs_s.device).type_as(xs_s)
    return out.permute(0, 2, 1)

def calculate_dpp_loss_single_objective(surrogate_model , generated_samples,last_cached_dpp_loss ,lambda_quality=0.5 , EPS =1e-6):
    """
        the loss funciton for calculating the diversity loss (dpp loss)
        inputs:
            -   surrogate_model : must be the instance  of residuaV0 with  correct  weigths
            -   generated_samples : scaled samples generated by diffusion model,must be flatten thhrough dim = 1 (batch ,  192 * 2)
            -   random_quality_weigths = simple torch.tensor weigth ditributed in gaussian. (N , 1)
        outputs : 
            -   single scalar the dpp loss (1,)

        ****** ATTENTION **********
        no need  for rescaling during this procedure
        ***************************
    """
    # generated_samples = inv_coords(generated_samples)
    gen_shape =  generated_samples.shape    #shape = (batch , 2, 192)

    temp_generated_samples = generated_samples.permute(0,2,1)  * 2 - 1    #shape = (batch , 192, 2)
    with torch.no_grad():
        labels = surrogate_model.get_cl_cd(temp_generated_samples) # (batch , 2)
    generated_samples = generated_samples.reshape(gen_shape[0], -1)
    cl_scaled ,cd_scaled = torch.chunk(labels ,chunks=2, dim =1)
    # define the quality
    quality = cl_scaled/(EPS +  cd_scaled)

    # Compute the similarity matrix
    x_2 = torch.sum(generated_samples ** 2 , dim=1  ,keepdim=True)
    difference_matrix = x_2 - 2 * (generated_samples @ generated_samples.t()) + x_2.t()
    samilarity_matrix = torch.exp(-(1/2) * (difference_matrix)) #gaussian kernel matrix

    # Compute the quality matrix
    quality = quality @ quality.T

    # Compute the dpp value
    if lambda_quality == 0 :
        dpp_L = samilarity_matrix
    else:
        dpp_L = samilarity_matrix * torch.pow(quality , lambda_quality)
    
    # calculating the eigen_values
    try:
        eig_vals = torch.linalg.eigh(dpp_L)[0]  # the  real value of the eigens
    except:
        return torch.tensor(last_cached_dpp_loss) , quality, samilarity_matrix , dpp_L
        
    loss = - torch.mean(torch.log(torch.clamp(eig_vals , min=EPS , max=1e2)))

    return loss , quality, samilarity_matrix , dpp_L

def calculate_dpp_loss_multi_objective(surrogate_model , generated_samples ,last_cached_dpp_loss, random_quality_weigths =[0.5,0.5] ,lambda_quality=0.5 , EPS =1e-6):
    """
        the loss funciton for calculating the diversity loss (dpp loss)
        inputs:
            -   surrogate_model : must be the instance  of residuaV0 with  correct  weigths
            -   generated_samples : scaled samples generated by diffusion model,must be flatten thhrough dim = 1 (batch ,  192 * 2)
            -   random_quality_weigths = simple torch.tensor weigth ditributed in gaussian. (N , 1)
        outputs : 
            -   single scalar the dpp loss (1,)

        ****** ATTENTION **********
        no need  for rescaling during this procedure
        ***************************

        here we want to use multi objective loss according to the definition,
        we want high CL ,low CD
        so we need  to use the Cl,-CD in stacking phase
        also the random_quality_weigths must be -> [Cl_weigths , Cd_weigths] such that the Cl_weight + Cd_weight = 1 (normalized value)
    """
    # generated_samples = inv_coords(generated_samples)
    random_quality_weigths = torch.tensor(random_quality_weigths, device=generated_samples.device).float()
    gen_shape =  generated_samples.shape   # shape = (batch , 2, 192) 
    temp_generated_samples = generated_samples.permute(0,2,1) * 2 - 1    #shape = (batch , 192, 2)
    with torch.no_grad():

        # labels = surrogate_model.get_cl_cd(temp_generated_samples.reshape(gen_shape[0], -1)) # (batch , 2)
        labels = surrogate_model(temp_generated_samples.reshape(gen_shape[0], -1)) # (batch , 2)

    generated_samples = generated_samples.reshape(gen_shape[0], -1)
    cl_scaled ,cd_scaled = torch.chunk(labels ,chunks=2, dim =1)
    # define the quality
    y = torch.stack([cl_scaled, (cl_scaled/cd_scaled + EPS)], dim=1)
    quality = (random_quality_weigths * y).sum(dim =1)

    # Compute the similarity matrix
    x_2 = torch.sum(generated_samples ** 2 , dim=1  ,keepdim=True)
    difference_matrix = x_2 - 2 * (generated_samples @ generated_samples.t()) + x_2.t()
    samilarity_matrix = torch.exp(-(1/2) * (difference_matrix)) #gaussian kernel matrix

    # Compute the quality matrix
    quality = quality @ quality.T

    # Compute the dpp value
    if lambda_quality == 0 :
        dpp_L = samilarity_matrix
    else:
        dpp_L = samilarity_matrix * torch.pow(quality , lambda_quality)
    
    # calculating the eigen_values
    try:
        eig_vals = torch.linalg.eigh(dpp_L)[0]  # the  real value of the eigens
    except:
        return torch.tensor(last_cached_dpp_loss)  , quality, samilarity_matrix , dpp_L
    loss = - torch.mean(torch.log(torch.clamp(eig_vals , min=EPS , max=1e2)))

    return loss , quality, samilarity_matrix , dpp_L

def train_diffusion_model(config):
    """
    get the config from the diffusion_config.yaml file
    """
    device       = torch.device(config["diffusion_train"]["device"] if torch.cuda.is_available() else "cpu")
    EPOCHS       = config["diffusion_train"]["EPOCHS"]
    LR           = config["diffusion_train"]["LR"]
    BATCH_SIZE   = config["diffusion_train"]["BATCH_SIZE"]

    NUM_WORKERS  = config["diffusion_train"]["NUM_WORKERS"]        # Kaggle usually has 2 CPUs
    PIN_MEMORY   = True        # copy tensors to pinned host memory → faster GPU transfer
    VIS_EPOCHS   = config["diffusion_train"]["VIS_EPOCHS"]
    MODEL_DIR    = config["diffusion_train"]["MODEL_DIR"]
    LOG_DIR = config["diffusion_train"]["LOG_DIR"]
    os.makedirs(MODEL_DIR,exist_ok=True)
    os.makedirs(LOG_DIR,exist_ok=True)
    dpp_epoch_tres = config["diffusion_train"]["dpp_epoch_tres"]
    cash_dpp_loss = config["diffusion_train"]["cash_dpp_loss"]
    LAMBDA_DPP = config["diffusion_train"]["LAMBDA_DPP"]
    LAMBDA_QUALITY = config["diffusion_train"]["LAMBDA_QUALITY"]
    DPP_WEIGTHS = config["diffusion_train"]["DPP_WEIGTHS"]
    DATA_DIR = config["data"]["BASE_DATA_DIR"]


    # ------------------------------------------------------------------ model
    model = Unet1D(
        dim        = 32,
        dim_mults  = (2,4,8,16),
        channels   = 2,
        dropout    = 0.1
    ).to(device)

    checkpoint_path = config["diffusion_train"]["Unet_starting_path"]
    model.load_state_dict(torch.load(checkpoint_path, weights_only=True))
    model.eval()

    diffusion = GaussianDiffusion1D(
        model,
        seq_length = 192,
        objective  = 'pred_noise',
        timesteps  = 1000,
        auto_normalize = True
    ).to(device)
    # # if we  updated the surrogates, then we should use their weigths, if not the weigths we provide to the yaml file
    # path_cd_model= os.path.join(config["surrogate_train"]["MODEL_DIR"],"best_cd_model.pt") if config["surrogate_train"]["train_flag"] == True else config["surrogate_model"]["cd_path"]
    # path_cl_model= os.path.join(config["surrogate_train"]["MODEL_DIR"],"best_cl_model.pt") if config["surrogate_train"]["train_flag"] == True else config["surrogate_model"]["cl_path"]
    
    # surrogate_model = Hybrid_surrogate_MLP(input_size=192 * 2, 
    #                                 hidden_layers_cd_model=[200,300,300,200],
    #                                 hidden_layers_cl_model=[150, 200,200,150],
    #                                 path_cd_model=path_cd_model,
    #                                 path_cl_model=path_cl_model
    #                                 ).to(device)
    if config["UA_surrogate_model"]["all_weigths_path"] != None:
        surrogate_model = Hybrid_surrogate_MLP(input_size=192 * 2, 
                                 hidden_layers_cd_model=[200,300,300,200],
                                 hidden_layers_cl_model=[150, 200,200,150],
                                 path_cd_model=config["UA_surrogate_model"]["init_cd_path"],
                                 path_cl_model=config["UA_surrogate_model"]["init_cl_path"]
                                 ).to(device)
    else:
        surrogate_model = UA_surrogate_model()
        surrogate_model = surrogate_model.load_state_dict(torch.load(config["UA_surrogate_model"]["all_weigths_path"],weights_only=True))
        surrogate_model = surrogate_model.to(config["UA_surrogate_model"]["device"])
        

    surrogate_model.eval()

    # ------------------------------------------------------------------ train stuff
    train_ds = AirfoilDataset(
        xs_scaled_path      = os.path.join(DATA_DIR,config["data"]["xs_train_path"]),
        ys_scaled_path      = os.path.join(DATA_DIR,config["data"]["ys_train_path"]),
        coord_min_max_path  = os.path.join(DATA_DIR,config["data"]["coord_min_max_path"]),
        label_min_max_path  = os.path.join(DATA_DIR,config["data"]["label_min_max_path"])
    )

    loader = DataLoader(
        train_ds,
        batch_size      = BATCH_SIZE,
        shuffle         = True,
        drop_last       = True,
        num_workers     = NUM_WORKERS,
        pin_memory      = PIN_MEMORY,
        persistent_workers = True      # keeps workers alive across epochs
    )

    optimizer = optim.AdamW(model.parameters(), lr=LR)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.75,
                                patience=10)#, verbose=True)
    loss_fn   = nn.L1Loss()
    writer    = SummaryWriter(log_dir=config["diffusion_train"]["LOG_DIR"])
    epoch_losses = []

    # ------------------------------------------------------------------ training
    model.train()
    for epoch in range(EPOCHS):
        running_loss = 0.0
        num_batch = 0
        pbar = tqdm(loader, desc=f"Epoch {epoch}/{EPOCHS-1}", leave=False)
        for coords_batch, _ in pbar:                      # labels unused for now
            coords_batch = coords_batch.to(device)        # (B,2,192)

            # sample random t and Gaussian noise
            t     = torch.randint(0, diffusion.num_timesteps,
                                (coords_batch.size(0),), device=device).long()
            noise = torch.randn_like(coords_batch)

            x_t  = diffusion.q_sample(coords_batch, t, noise=noise)
            pred = diffusion.model(x_t, t)
            # calculating the dpp loss
            if num_batch % 2 == (epoch %2):
                model.eval()
                with torch.no_grad():
                    generated_samples = diffusion.latent_sample(x_t , is_ddim=True)
                dpp_loss = calculate_dpp_loss_multi_objective(surrogate_model=surrogate_model,
                                                    generated_samples=generated_samples,
                                                    last_cached_dpp_loss=cash_dpp_loss,
                                                    random_quality_weigths=DPP_WEIGTHS,
                                                    lambda_quality=LAMBDA_QUALITY,
                                                    )[0]
                model.train()
                cash_dpp_loss = dpp_loss.item()
            loss = loss_fn(noise, pred) + LAMBDA_DPP * cash_dpp_loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.4f}")
            num_batch += 1
            # break
        # # Calculating the dpp loss
        # if epoch> dpp_epoch_tres:
        #     model.eval()
        #     with torch.no_grad():
        #         samples = diffusion.sample(batch_size=128)  # sample 4 airfoils
        #     generated_samples = samples
        #     dpp_loss = calculate_dpp_loss_multi_objective(surrogate_model=surrogate_model,
        #                                         generated_samples=generated_samples,
        #                                         last_cached_dpp_loss=cash_dpp_loss,
        #                                         random_quality_weigths=DPP_WEIGTHS,
        #                                         lambda_quality=LAMBDA_QUALITY,
        #                                         )[0]
        #     cash_dpp_loss = dpp_loss.item()
        avg_loss = running_loss / len(loader)
        epoch_losses.append(avg_loss)
        writer.add_scalar("Loss/train", avg_loss, epoch)
        scheduler.step(avg_loss)

        # ---------------- visual samples every VIS_EPOCHS ------------------
        if (epoch % VIS_EPOCHS == 0) or (epoch == EPOCHS-1):
            model.eval()
            with torch.no_grad():
                samples_s = diffusion.sample(batch_size=4)      # (4,2,192) scaled
            samples_r = train_ds.inverse_scale_coords(
                            samples_s.permute(0,2,1).cpu().numpy()).transpose(0,2,1)

            fig, ax = plt.subplots(1,4, figsize=(12,3))
            for i in range(4):
                ax[i].plot(samples_r[i,0], samples_r[i,1])
                ax[i].axis('equal'); ax[i].axis('off'); ax[i].set_title(f"S{i}")
            plt.suptitle(f"Samples at epoch {epoch}")
            plt.savefig(os.path.join(LOG_DIR, f"samples_epoch_{epoch}.png"))
            writer.add_figure("Samples", fig, epoch)

        # ---------------- save checkpoint every 25 epochs ------------------
        if (epoch % 25 == 0) or (epoch == EPOCHS-1):
            torch.save(model.state_dict(), os.path.join(MODEL_DIR , f"model_epoch_{epoch}.pt"))
            plt.figure(figsize=(8,5))
            plt.plot(epoch_losses, label="train loss")
            plt.xlabel("epoch"); plt.ylabel("L1"); plt.title("Training Loss"); plt.legend();
            plt.savefig(os.path.join(MODEL_DIR , "loss_map.png"))
            

    writer.close()

    # ---------------- plot loss curve ------------------
    plt.figure(figsize=(8,5))
    plt.plot(epoch_losses, label="train loss")
    plt.xlabel("epoch")
    plt.ylabel("L1")
    plt.title("Training Loss")
    plt.legend()
    plt.savefig(os.path.join(LOG_DIR,"loss.png"))

def train_surrogate_model(config):
    if config["surrogate_train"]["train_flag"] == False:
        print("Skip the surrogate updating")
        return 
    

    input_size = config["surrogate_train"]["input_size"]
    num_epochs = config["surrogate_train"]["EPOCHS"]
    learning_rate = config["surrogate_train"]["LR"]
    learning_rate_decay = 0.999
    reg = 0.001
    batch_size = config["surrogate_train"]["BATCH_SIZE"]
    device = config["surrogate_train"]["device"] if torch.cuda.is_available() else "cpu"
    num_workers = config["surrogate_train"]["NUM_WORKERS"]
    patience = config["surrogate_train"]["patience"]
    MODEL_DIR = config["surrogate_train"]["MODEL_DIR"]
    LOG_DIR = config["surrogate_train"]["LOG_DIR"]
    DATA_DIR = config["data"]["BASE_DATA_DIR"]
    os.makedirs(MODEL_DIR,exist_ok=True)
    os.makedirs(LOG_DIR,exist_ok=True)

    xs_train = np.load(os.path.join(DATA_DIR,config["data"]["xs_train_path"]))
    ys_train = np.load(os.path.join(DATA_DIR,config["data"]["ys_train_path"]))

    xs_train = xs_train.reshape(xs_train.shape[0],-1)

    x_train_tensor = torch.from_numpy(xs_train).float()
    y_train_tensor = torch.from_numpy(ys_train).float()
    

    dataset = TensorDataset(x_train_tensor, y_train_tensor)

    lengths = [int(len(dataset)*0.9), len(dataset)-int(len(dataset)*0.9)]

    train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths)
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                            batch_size=batch_size , 
                                            num_workers=  num_workers,
                                            drop_last= True , 
                                            shuffle = True)
    test_loader = torch.utils.data.DataLoader(dataset=val_dataset, 
                                            batch_size=batch_size , 
                                            num_workers = num_workers)



    # model_mlp = MultiLayerPerceptronWithCustomAttention(input_size, hidden_size, num_classes,3).to(device)
    # model_mlp = Hybrid_surrogate_MLP(input_size=config["surrogate_train"]["input_size"], 
    #                                 hidden_layers_cd_model=config["surrogate_train"]["hidden_layers_cd_model"],
    #                                 hidden_layers_cl_model=config["surrogate_train"]["hidden_layers_cl_model"],
    #                                 path_cd_model=config["surrogate_train"]["cd_path"],
    #                                 path_cl_model=config["surrogate_train"]["cl_path"]
    #                                 ).to(device)
    if config["UA_surrogate_model"]["all_weigths_path"] !=None:
        model_mlp = Hybrid_surrogate_MLP(input_size=192 * 2, 
                                 hidden_layers_cd_model=[200,300,300,200],
                                 hidden_layers_cl_model=[150, 200,200,150],
                                 path_cd_model=config["UA_surrogate_model"]["init_cd_path"],
                                 path_cl_model=config["UA_surrogate_model"]["init_cl_path"],
                                 net_n_cd=3,
                                 net_n_cl=3
                                 ).to(device)
    else:
        model_mlp = UA_surrogate_model()
        model_mlp = model_mlp.load_state_dict(torch.load(config["UA_surrogate_model"]["all_weigths_path"],weights_only=True))
        model_mlp = model_mlp.to(config["UA_surrogate_model"]["device"])
        
        


    criterion_train= nn.L1Loss()
    criterion_val   = nn.L1Loss()  # Standard mean MSE
    optimizer = torch.optim.Adam(model_mlp.parameters(), lr=learning_rate)

    ########################################
    # 5) Training loop with early stopping
    ########################################
    best_loss = float('inf')
    best_epoch = 0
    epochs_no_improve = 0
    lr_current = learning_rate

    train_losses = []
    test_losses  = []

    start_time = time.time()

    for epoch in range(1, num_epochs + 1):
        # ---- Training ----
        model_mlp.train()
        running_loss = 0.0
        for batch_idx, (features, labels) in enumerate(train_loader):
            features, labels = features.to(device), labels.to(device)
            # print(f"{features.shape=}")
            # print(f"{labels.shape=}")
            optimizer.zero_grad()
            outputs = model_mlp(features)  # shape (batch, 2)
            outputs *= 1000
            labels[:,-1] *= 1000  #cd
            labels[:,0] *= 1000   #cl
            loss = config["surrogate_train"]["LAMBDA_CD"] * criterion_train(outputs[:,1], labels[:,1]) #cd
            # loss =0 
            loss += config["surrogate_train"]["LAMBDA_CL"] * criterion_train(outputs[:,0], labels[:,0])   #cl
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * features.size(0)

            # if (batch_idx + 1) % 100 == 0:
            #     print(f"Epoch [{epoch}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}],"
            #           f" Train Loss (batch): {loss.item():.4f}")

        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)

        # ---- Validation ----
        model_mlp.eval()
        test_loss = 0.0
        with torch.no_grad():
            for features, labels in test_loader:
                features, labels = features.to(device), labels.to(device)
                preds = model_mlp(features)
                # print(f"{preds.shape=}")
                # print(f"{labels[:,1].shape=}")
                # sys.exit()
                test_loss += (criterion_val(preds[:,1], labels[:,1]) + 
                              criterion_val(preds[:,0], labels[:,0])).item() * features.size(0)
                # test_loss += (criterion_val(preds[:,0], labels[:,0])).item() * features.size(0)
        epoch_test_loss = test_loss / len(test_loader.dataset)
        test_losses.append(epoch_test_loss)

        # Print log
        print(f"Epoch [{epoch}/{num_epochs}] - Train Loss: {epoch_train_loss:.6f}, Test Loss: {epoch_test_loss:.6f}")

        # Early stopping check
        if epoch_test_loss < best_loss:
            best_loss = epoch_test_loss
            best_epoch = epoch
            epochs_no_improve = 0
            # saving both cl , cd best paths.
            torch.save(model_mlp.cl_forward_mlp.state_dict(), os.path.join(config["surrogate_train"]["MODEL_DIR"],"best_cl_model.pt"))
            torch.save(model_mlp.cd_forward_mlp.state_dict(), os.path.join(config["surrogate_train"]["MODEL_DIR"],"best_cd_model.pt"))
            print(f"New best model at epoch {epoch} with test loss {best_loss:.6f}")
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print(f"No improvement for {patience} epochs. Early stopping at epoch {epoch}.")
                break

        # Update learning rate
        lr_current *= learning_rate_decay
        update_lr(optimizer, lr_current)

    elapsed = time.time() - start_time
    print(f"\nTraining completed in {elapsed:.2f} seconds. Best epoch: {best_epoch} with test loss {best_loss:.6f}")


    ############################################################
    # 6) Save and plot train/test losses in separate subplots
    ############################################################
    train_losses = np.array(train_losses)
    test_losses  = np.array(test_losses)

    # Save them as .npy
    np.save(os.path.join(config["surrogate_train"]["LOG_DIR"],"train_losses.npy"), train_losses)
    np.save(os.path.join(config["surrogate_train"]["LOG_DIR"],"test_losses.npy"),  test_losses)

    # Two subplots: one for Train Loss, one for Test Loss
    fig, axs = plt.subplots(1, 2, figsize=(12, 4))

    axs[0].plot(train_losses, label="Train Loss", color="blue")
    axs[0].set_title("Train Loss vs Epoch")
    axs[0].set_xlabel("Epoch")
    axs[0].set_ylabel("Loss")
    axs[0].grid(True, linestyle="--", alpha=0.7)
    axs[0].legend()

    axs[1].plot(test_losses, label="Test Loss", color="orange")
    axs[1].set_title("Test Loss vs Epoch")
    axs[1].set_xlabel("Epoch")
    axs[1].set_ylabel("Loss")
    axs[1].grid(True, linestyle="--", alpha=0.7)
    axs[1].legend()

    plt.tight_layout()
    plt.savefig(os.path.join(LOG_DIR,"losses.png"))

# ----------------------------------------------------------------------
if __name__ == "__main__":
    with open("src/optimization_loop/train_diffusion/diffusion_config.yaml", "r") as file:
        config = yaml.safe_load(file)  # Converts YAML → Python dict

    # Stage 1:  wheter if we need to update the surrogate model
    os.makedirs("src/optimization_loop/train_diffusion/weigths",exist_ok=True)
    print("Stage 1")
    train_surrogate_model(config)
    # Stage 2: updating the diffusion model
    print("Stage 1")
    train_diffusion_model(config)
    print("End of the Diffusion training")
