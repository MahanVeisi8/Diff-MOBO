{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os , sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from: /home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\n"
     ]
    }
   ],
   "source": [
    "from Gen_src.diffusion import GaussianDiffusion1D\n",
    "from Gen_src.model import Unet1D\n",
    "import torch\n",
    "import os,sys\n",
    "# Add the parent directory of Gen_src to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Same architecture as in training\n",
    "model = Unet1D(\n",
    "    dim=32,\n",
    "    dim_mults=(2, 4, 8, 16),\n",
    "    channels=2,  # X and Y\n",
    "    dropout=0.1\n",
    ").cuda()  # or .to(device)\n",
    "\n",
    "# Create the same diffusion wrapper\n",
    "diffusion = GaussianDiffusion1D(\n",
    "    model,\n",
    "    seq_length=192,      # must match your training setup\n",
    "    objective='pred_noise',\n",
    "    timesteps=1000,\n",
    "    auto_normalize=False\n",
    ").cuda()  # or .to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "model.eval()\n",
    "print(\"Loaded model weights from:\", checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/data/raw\")\n",
    "coord_mm = np.load(DATA_DIR/\"coord_min_max.npy\")  # [[x_min,y_min],[x_max,y_max]]\n",
    "x_min,y_min = coord_mm[0]; x_max,y_max = coord_mm[1]\n",
    "\n",
    "def inv_coords(xs_s):                   # xs_s shape (...,2,192) tensor\n",
    "    xs_np = xs_s.permute(0,2,1).cpu().numpy()    # -> (B,192,2)\n",
    "    xs_np[...,0] = xs_np[...,0]*(x_max-x_min) + x_min\n",
    "    xs_np[...,1] = xs_np[...,1]*(y_max-y_min) + y_min\n",
    "    return xs_np                                # (B,192,2) numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:12<00:00, 77.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_to_generate = 2\n",
    "batch_size      = 2\n",
    "\n",
    "# generated_scaled = []\n",
    "# latent = torch.randn((1,2,192)).to(\"cuda\")\n",
    "# samples1 = diffusion.latent_sample(latent , is_ddim=True)\n",
    "# samples2 = diffusion.latent_sample(latent , is_ddim=True)\n",
    "# generated_samples1 = inv_coords(samples1)\n",
    "# generated_samples2 = inv_coords(samples2)\n",
    "\n",
    "\n",
    "all_latent = []\n",
    "all_shapes = []\n",
    "with torch.no_grad():\n",
    "    done = 0\n",
    "    while done < num_to_generate:\n",
    "        cur = min(batch_size, num_to_generate - done)\n",
    "        \n",
    "        latent = torch.randn((cur,2,192)).to(\"cuda\")\n",
    "        samples = diffusion.latent_sample(latent , is_ddim=True)\n",
    "        generated_real = inv_coords(samples)\n",
    "        \n",
    "        all_latent.append(latent.cpu().detach().numpy())\n",
    "        all_shapes.append(generated_real)\n",
    "        done += cur\n",
    "        print(f\"Generated {done}/{num_to_generate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_mount_path = \"/home/bardiya/projects/diffusion_air_manifolding/codes/Airfoil_MPI_system\"\n",
    "# docker_mount_path = \".\"\n",
    "\n",
    "np.save(os.path.join(docker_mount_path , \"DB2.npy\") , {\n",
    "    \"latents\": np.vstack(all_latent),\n",
    "    \"shapes\": np.vstack(all_shapes),\n",
    "    \"performances\": None\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 192, 2)\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9800665511016924, -0.004483645182613369, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9800665511016924, -0.004483645182613369, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9800665511016924, -0.004483645182613369, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9800665511016924, -0.004483645182613369, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9800665511016924, -0.004483645182613369, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9800665511016924, -0.004483645182613369, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9800665511016924, -0.004483645182613369, 1) in surface 1026\n",
      "Error   : Unable to recover the edge 1885 (15/452) on GEdge 1000 (on GFace 1026)\n",
      "Warning : Surface 1 consists of no elements\n",
      "Warning : Volume 1 consists of no elements\n",
      "Error   : ------------------------------\n",
      "Error   : Mesh generation error summary\n",
      "Error   :    43 warnings\n",
      "Error   :     8 errors\n",
      "Error   : Check the full log for details\n",
      "Error   : ------------------------------\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/airfoil_UANA/Airfoil_simulation_1/ShapeToPerformance.py\", line 211, in calculate_cd_cl_res\n",
      "    if runSim(fsX, fsY) != 0:\n",
      "  File \"/home/airfoil_UANA/Airfoil_simulation_1/ShapeToPerformance.py\", line 89, in runSim\n",
      "    with open(\"U_template\", \"rt\") as inFile:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'U_template'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core 1:\n",
      "sample 1:\n",
      "\tUsing len 40.000 angle +0.000 \n",
      "\tResulting freestream vel x,y: 40.0,-0.0\n",
      "error during mesh creation!\n",
      "\tmesh generation failed, aborting\n",
      "-1000\n",
      "1000\n",
      "/home/airfoil_UANA\n",
      "/home/airfoil_UANA\n",
      "core 0:\n",
      "sample 0:\n",
      "\tUsing len 40.000 angle +0.000 \n",
      "\tResulting freestream vel x,y: 40.0,-0.0\n",
      "/home/airfoil_UANA/Airfoil_simulation_1/OpenFOAM_0\n",
      "/home/airfoil_UANA/Airfoil_simulation_1/OpenFOAM_0\n",
      "0.2474784\n",
      "0.008937957\n",
      "0.txt\n",
      "1.txt\n",
      "[0 1]\n",
      "[[ 2.47478e-01  8.93800e-03  0.00000e+00]\n",
      " [-1.00000e+03  1.00000e+03  1.00000e+00]]\n",
      "done!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append(\"../OpenFoam\")\n",
    "from OpenFoam.Airfoil_simulation_1.ShapeToPerformance import shape_to_performance as STP1\n",
    "docker_container_id = \"78257c051b5b\" \n",
    "# command = fr\"docker exec {docker_container_id} python3 /home/airfoil_UANA/performance_finding.py\"\n",
    "command = (\n",
    "    f'docker exec {docker_container_id} bash -c \"'\n",
    "    f'source /opt/openfoam5/etc/bashrc && '\n",
    "    f'cd /home/airfoil_UANA && '\n",
    "    f'python3 performance_finding.py\"'\n",
    ")\n",
    "\n",
    "os.system(command)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.47478e-01,  8.93800e-03,  0.00000e+00],\n",
       "       [-1.00000e+03,  1.00000e+03,  1.00000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_path =os.path.join(docker_mount_path , rf\"performance.npy\")\n",
    "perfromance = np.load(performance_path,allow_pickle=True)\n",
    "perfromance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "sys.path.append(\"../surrogate_models\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "activation_function_list = [torch.tanh, nn.ReLU(), nn.CELU(), nn.LeakyReLU(), nn.ELU(), nn.Hardswish(),torch.tanh, nn.ReLU(), nn.CELU(), nn.LeakyReLU(), torch.tanh]\n",
    "\n",
    "class MultiLayerPerceptron_forward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, net_n):\n",
    "        super(MultiLayerPerceptron_forward, self).__init__()\n",
    "        #################################################################################\n",
    "        # Initialize the modules required to implement the mlp with given layer   #\n",
    "        # configuration. input_size --> hidden_layers[0] --> hidden_layers[1] .... -->  #\n",
    "        # hidden_layers[-1] --> num_classes                                             #\n",
    "        #################################################################################\n",
    "        layers = []\n",
    "        layers.append(nn.Linear((input_size), (hidden_layers[0])))\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            layers.append(nn.Linear((hidden_layers[i]), (hidden_layers[i+1])))\n",
    "\n",
    "        layers.append(nn.Linear((hidden_layers[len(hidden_layers)-1]), (num_classes)))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.net_n = net_n\n",
    "        self.hidden_layers = hidden_layers\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        m = activation_function_list[self.net_n]\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            x = self.layers[i](x)\n",
    "            x = m(x)\n",
    "        x = (self.layers[len(self.hidden_layers)](x))\n",
    "        out=x\n",
    "        return out\n",
    "\n",
    "#====================================================\n",
    "\n",
    "class Hybrid_surrogate_MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_layers_cl_model ,\n",
    "                 hidden_layers_cd_model , \n",
    "                 net_n_cl=3 , \n",
    "                 net_n_cd=3, \n",
    "                 path_cl_model = None, \n",
    "                 path_cd_model  =  None):\n",
    "        super(Hybrid_surrogate_MLP, self).__init__()\n",
    "        self.cl_forward_mlp = MultiLayerPerceptron_forward(input_size , hidden_layers_cl_model ,   num_classes=1  , net_n=net_n_cl)\n",
    "        self.cd_forward_mlp = MultiLayerPerceptron_forward(input_size , hidden_layers_cd_model ,   num_classes=1  , net_n=net_n_cd)\n",
    "        if path_cl_model:\n",
    "            self.cl_forward_mlp.load_state_dict(torch.load(path_cl_model,map_location=\"cpu\"))\n",
    "        if path_cd_model:\n",
    "            self.cd_forward_mlp.load_state_dict(torch.load(path_cd_model,map_location=\"cpu\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        \"\"\"\n",
    "            x.shape= (batch , 192, 2)\n",
    "        \"\"\"\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        cl = self.cl_forward_mlp(x)\n",
    "        cd = self.cd_forward_mlp(x)\n",
    "        return torch.stack([cl,cd],dim=1).squeeze(-1)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best Residual surrogate model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3071/298942254.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.cl_forward_mlp.load_state_dict(torch.load(path_cl_model,map_location=\"cpu\"))\n",
      "/tmp/ipykernel_3071/298942254.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.cd_forward_mlp.load_state_dict(torch.load(path_cd_model,map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "residv1_best_model = Hybrid_surrogate_MLP(input_size=192 * 2, \n",
    "                                 hidden_layers_cd_model=[200,300,300,200],\n",
    "                                 hidden_layers_cl_model=[150, 200,200,150],\n",
    "                                 path_cd_model=rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/surrogate_models/Surrogate_Model_weigths/just_for_cd_200_300_300_200/mlp_best_model.pt\",\n",
    "                                 path_cl_model=rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/surrogate_models/Surrogate_Model_weigths/just_for_cl_150_200_200_150/mlp_best_model.pt\"\n",
    "                                 ).to(\"cuda\")\n",
    "\n",
    "# Load the checkpoint\n",
    "# residv1_best_model.load_state_dict(torch.load(checkpoint_dir, map_location=device))\n",
    "residv1_best_model.eval()\n",
    "print(\"Loaded best Residual surrogate model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 192, 2), numpy.ndarray)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_shapes[0].shape ,type(all_shapes[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5923, 0.0068],\n",
       "        [0.5673, 0.0078]], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = torch.from_numpy(all_shapes[0]).to(\"cuda\")\n",
    "ans = residv1_best_model(temp)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
