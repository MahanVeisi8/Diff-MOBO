{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import  os, sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bardiya/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/optimization_loop/Inner_loop/../../diffusion_core/diffusion.py:331: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"src/\")\n",
    "sys.path.append(\"src/OpenFoam\")\n",
    "sys.path.append(\"src/diffusion_notebooks\")\n",
    "sys.path.append(\"data/\")\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from diffusion_core.diffusion import GaussianDiffusion1D\n",
    "from diffusion_core.model import Unet1D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.2752, -0.0072],\n",
      "        [-0.2752, -0.0072]], grad_fn=<CatBackward0>), tensor([[-0.1909, -0.0035],\n",
      "        [-0.1909, -0.0035]], grad_fn=<CatBackward0>), tensor([[0.0310, 1.6393],\n",
      "        [0.0310, 1.6393]], grad_fn=<CatBackward0>), tensor([[-0.2294,  0.0017],\n",
      "        [-0.2294,  0.0017]], grad_fn=<CatBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "from models import UA_surrogate_model\n",
    "\n",
    "if __name__  == \"__main__\":\n",
    "    model = UA_surrogate_model()\n",
    "    x = torch.zeros((2,384))\n",
    "    out = model(x)\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\"\n",
    "# Same architecture as in training\n",
    "model = Unet1D(\n",
    "    dim=32,\n",
    "    dim_mults=(2, 4, 8, 16),\n",
    "    channels=2,  # X and Y\n",
    "    dropout=0.1\n",
    ").to(device)  # or .to(device)\n",
    "\n",
    "# Create the same diffusion wrapper\n",
    "diffusion = GaussianDiffusion1D(\n",
    "    model,\n",
    "    seq_length=192,      # must match your training setup\n",
    "    objective='pred_noise',\n",
    "    timesteps=1000,\n",
    "    auto_normalize=False\n",
    ").to(device)  # or .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from utils import  BO_surrogate_uncertainty\n",
    "sys.path.append(\"../../OpenFoam\")\n",
    "from OpenFoam.Airfoil_simulation_1.ShapeToPerformance import shape_to_performance as STP1\n",
    "\n",
    "DATA_DIR = Path(rf\"../../../data\")\n",
    "coord_mm = np.load(DATA_DIR/\"coord_min_max.npy\")  # [[x_min,y_min],[x_max,y_max]]\n",
    "x_min,y_min = coord_mm[0]; x_max,y_max = coord_mm[1]\n",
    "\n",
    "def inv_coords(xs_s):                   # xs_s shape (...,2,192) tensor\n",
    "    xs_np = xs_s.permute(0,2,1).cpu().numpy()    # -> (B,192,2)\n",
    "    xs_np[...,0] = xs_np[...,0]*(x_max-x_min) + x_min\n",
    "    xs_np[...,1] = xs_np[...,1]*(y_max-y_min) + y_min\n",
    "    return xs_np                                # (B,192,2) numpy\n",
    "\n",
    "def init_generate_samples_latents(model ,diffusion, checkpoint_path, NUM_TO_GENERATE , BATCH_SIZE):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            checkpoint_path:    the loading path for the weigths of the\n",
    "                                diffusion unet.\n",
    "            NUM_TO_GENERATE\n",
    "            BATCH_SIZE\n",
    "        output: \n",
    "            all_latents:    list of the latents used for sampling phase\n",
    "            all_shapes:     list of the shapes  generated from the latents\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    # checkpoint_path = rf\"src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\"\n",
    "    model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "    model.eval()\n",
    "    print(\"Loaded model weights from:\", checkpoint_path)\n",
    "\n",
    "    num_to_generate = NUM_TO_GENERATE\n",
    "    batch_size      = BATCH_SIZE\n",
    "\n",
    "    all_latent = []\n",
    "    all_shapes = []\n",
    "    with torch.no_grad():\n",
    "        done = 0\n",
    "        while done < num_to_generate:\n",
    "            cur = min(batch_size, num_to_generate - done)\n",
    "            \n",
    "            latent = torch.randn((cur,2,192)).to(device)\n",
    "            samples = diffusion.latent_sample(latent , is_ddim=True)\n",
    "            generated_real = inv_coords(samples)\n",
    "            \n",
    "            all_latent.append(latent.cpu().detach().numpy())\n",
    "            all_shapes.append(generated_real)\n",
    "            done += cur\n",
    "            print(f\"Generated {done}/{num_to_generate}\")\n",
    "\n",
    "    np.save(os.path.join(\"Database\" , \"DB_innerloop.npy\") , {\n",
    "        \"latents\": np.vstack(all_latent),\n",
    "        \"shapes\": np.vstack(all_shapes),\n",
    "    })\n",
    "\n",
    "def GEN_UA(number_iter = 0,number_generations=100 , population_size = 1000 , from_DB_innerloop = True):\n",
    "    # n_iter = 2\n",
    "    print('calculating surrogate pareto ...')\n",
    "    if from_DB_innerloop:\n",
    "        DB_innerloop = np.load(os.path.join(\"Database\" , \"DB_innerloop.npy\"),allow_pickle=True).item()\n",
    "        full_sampels = DB_innerloop[\"latents\"].reshape(DB_innerloop[\"latents\"].shape[0], -1)  # shape: (batch, 384)\n",
    "        print(full_sampels.shape)\n",
    "    problem_uncertainty = BO_surrogate_uncertainty(n_iter=number_iter)\n",
    "    algorithm = NSGA2(pop_size=population_size)\n",
    "    res = minimize(problem_uncertainty,\n",
    "                algorithm,\n",
    "                ('n_gen', number_generations),\n",
    "                seed=1,\n",
    "                verbose=False,\n",
    "                X = full_sampels  if from_DB_innerloop else None)\n",
    "    Paretoset_uncertainty = res.X\n",
    "    Out_surrogate_uncertainty = res.F\n",
    "    # sio.savemat('surrogate_pareto/ParetoSet_test.mat' , {'ParetoSet': np.array(Paretoset_uncertainty)})\n",
    "    # sio.savemat('surrogate_pareto/Out_surrogate_test.mat', {'Out_surrogate': np.array(Out_surrogate_uncertainty)})\n",
    "    np.save(os.path.join(\"Database\" , \"DB_NSGA.npy\"),{\n",
    "        'ParetoSet': np.array(Paretoset_uncertainty), # the latents\n",
    "        'Out_surrogate': np.array(Out_surrogate_uncertainty)\n",
    "    })\n",
    "\n",
    "def NSGA_latent_to_shape(model ,diffusion, docker_mount_path, checkpoint_path, BATCH_SIZE=128):\n",
    "\n",
    "    DB_NSGA = np.load(os.path.join(\"Database\" , \"DB_NSGA.npy\"),allow_pickle=True).item()\n",
    "\n",
    "    # reset the latent to (batch,2,192)\n",
    "    NSGA_latent = DB_NSGA[\"ParetoSet\"].reshape(DB_NSGA[\"ParetoSet\"].shape[0],2,-1)\n",
    "    NSGA_latent = torch.from_numpy(NSGA_latent).to(torch.float32)\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "    model.eval()\n",
    "    print(\"Loaded model weights from:\", checkpoint_path)\n",
    "\n",
    "    num_to_generate = len(NSGA_latent)\n",
    "    batch_size = BATCH_SIZE\n",
    "\n",
    "    all_latent = []\n",
    "    all_shapes = []\n",
    "    with torch.no_grad():\n",
    "        done = 0\n",
    "        while done < num_to_generate:\n",
    "            cur = min(batch_size, num_to_generate - done)\n",
    "            \n",
    "            latent = NSGA_latent[done:done+cur,:,:].to(device)\n",
    "            samples = diffusion.latent_sample(latent , is_ddim=True)\n",
    "            generated_real = inv_coords(samples)\n",
    "            \n",
    "            all_latent.append(latent.cpu().detach().numpy())\n",
    "            all_shapes.append(generated_real)\n",
    "            done += cur\n",
    "            print(f\"Generated {done}/{num_to_generate}\")\n",
    "\n",
    "    np.save(os.path.join(docker_mount_path , \"DB_CFD.npy\") , {\n",
    "        \"latents\": np.vstack(all_latent),\n",
    "        \"shapes\": np.vstack(all_shapes),\n",
    "        \"performance\":[]\n",
    "    })\n",
    "\n",
    "def CFD_simulation(docker_container_id):\n",
    "    # command = fr\"docker exec {docker_container_id} python3 /home/airfoil_UANA/performance_finding.py\"\n",
    "    command = (\n",
    "        f'docker exec {docker_container_id} bash -c \"'\n",
    "        f'source /opt/openfoam5/etc/bashrc && '\n",
    "        f'cd /home/airfoil_UANA && '\n",
    "        f'python3 innerloop_performance_finding.py\"'\n",
    "    )\n",
    "    os.system(command)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n******************************\\n******************************\\nwarning:\\n    before running make sure to copy src/OpenFoam/Airfoil_simulation_1/OpenFOAM_0\\n    in that folder 200 times with new directories name as src/OpenFoam/Airfoil_simulation_1/OpenFOAM_i for the i'th core ussage\\n******************************\\n******************************\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The main Hyperparams\n",
    "\"\"\"\n",
    "docker_mount_path = \"../../OpenFoam\"\n",
    "NUM_TO_GENERATE = 2\n",
    "BATCH_SIZE = 2\n",
    "docker_container_id = \"f897792b6b56\" \n",
    "saving_path = rf\"src/optimization_loop/Inner_loop/Database/DB_innerloop.npy\"\n",
    "Unet_checkpoint_path = rf\"../../../src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\"\n",
    "\"\"\"\n",
    "******************************\n",
    "******************************\n",
    "warning:\n",
    "    before running make sure to copy src/OpenFoam/Airfoil_simulation_1/OpenFOAM_0\n",
    "    in that folder 200 times with new directories name as src/OpenFoam/Airfoil_simulation_1/OpenFOAM_i for the i'th core ussage\n",
    "******************************\n",
    "******************************\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Procedure\n",
    "1.  creating and generating the `DB_innerloop` for the shapes and latents holding (just these two) ->  OK\n",
    "2.  making the `UA_information` data for the whole `DB_innerloop` and save it in `DB_UA` for the NSGA algorithm ->  OK\n",
    "3.  using the `NSGA` algorithm and  creating the `DB_NSGA` -> OK\n",
    "4.  give the results of the `DB_NSGA` to the Openfoam and  create `DB_OpenFoam`\n",
    "5.  evaluate and tag the `DB_Openfoam` and append the correct data to the `DB_Valid` and `DB_Invalid`\n",
    "6.  retrain the `UA_surrogate_Model` with the valid datas\n",
    "7.  redo  all above part for `N` Epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stage 1:\n",
    "# init_generate_samples_latents(model , \n",
    "#                             diffusion , \n",
    "#                             NUM_TO_GENERATE=NUM_TO_GENERATE,\n",
    "#                             BATCH_SIZE=BATCH_SIZE,\n",
    "#                             checkpoint_path=Unet_checkpoint_path\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating surrogate pareto ...\n",
      "(2, 384)\n",
      "\n",
      "Compiled modules for significant speedup can not be used!\n",
      "https://pymoo.org/installation.html#installation\n",
      "\n",
      "To disable this warning:\n",
      "from pymoo.config import Config\n",
      "Config.warnings['not_compiled'] = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from utils import  BO_surrogate_uncertainty\n",
    "\n",
    "# Stage 2:\n",
    "GEN_UA(number_generations=1, population_size=5,from_DB_innerloop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from: ../../../src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:26<00:00, 37.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:14<00:00, 71.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Stage 3:\n",
    "NSGA_latent_to_shape(model , \n",
    "                diffusion , \n",
    "                BATCH_SIZE=BATCH_SIZE,\n",
    "                checkpoint_path=Unet_checkpoint_path,\n",
    "                docker_mount_path=docker_mount_path\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4 (first  start it):\n",
    "CFD_simulation(docker_container_id=docker_container_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hold = np.load(os.path.join( docker_mount_path, \"DB_CFD.npy\"),allow_pickle=True).item()\n",
    "a = hold[\"performance\"]\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1000.,  1000.,     0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
