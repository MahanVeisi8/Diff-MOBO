{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as  pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import  os, sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bardiya/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/optimization_loop/Inner_loop/../../diffusion_core/diffusion.py:331: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"src/\")\n",
    "sys.path.append(\"src/OpenFoam\")\n",
    "sys.path.append(\"src/diffusion_notebooks\")\n",
    "sys.path.append(\"data/\")\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from diffusion_core.diffusion import GaussianDiffusion1D\n",
    "from diffusion_core.model import Unet1D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from models import UA_surrogate_model\n",
    "\n",
    "if __name__  == \"__main__\":\n",
    "    model = UA_surrogate_model()\n",
    "    x = torch.zeros((2,384))\n",
    "    out = model(x)\n",
    "    out = torch.stack(out,dim=0)\n",
    "    print(out.shape)\n",
    "    k = torch.mean(out,dim=0)\n",
    "    print(k.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\"\n",
    "# device = \"cuda\"\n",
    "# Same architecture as in training\n",
    "Unet_model = Unet1D(\n",
    "    dim=32,\n",
    "    dim_mults=(2, 4, 8, 16),\n",
    "    channels=2,  # X and Y\n",
    "    dropout=0.1\n",
    ").to(device)  # or .to(device)\n",
    "\n",
    "# Create the same diffusion wrapper\n",
    "diffusion = GaussianDiffusion1D(\n",
    "    Unet_model,\n",
    "    seq_length=192,      # must match your training setup\n",
    "    objective='pred_noise',\n",
    "    timesteps=1000,\n",
    "    auto_normalize=False\n",
    ").to(device)  # or .to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from utils import  BO_surrogate_uncertainty\n",
    "sys.path.append(\"../../OpenFoam\")\n",
    "from OpenFoam.Airfoil_simulation_1.ShapeToPerformance import shape_to_performance as STP1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "DATA_DIR = Path(rf\"../../../data\")\n",
    "coord_mm = np.load(DATA_DIR/\"coord_min_max.npy\")  # [[x_min,y_min],[x_max,y_max]]\n",
    "x_min,y_min = coord_mm[0]; x_max,y_max = coord_mm[1]\n",
    "print(x_min,y_min)\n",
    "\n",
    "def inv_coords(xs_s):                   # xs_s shape (...,2,192) tensor\n",
    "    xs_np = xs_s.permute(0,2,1).cpu().numpy()    # -> (B,192,2)\n",
    "    xs_np[...,0] = xs_np[...,0]*(x_max-x_min) + x_min\n",
    "    xs_np[...,1] = xs_np[...,1]*(y_max-y_min) + y_min\n",
    "    return xs_np                                # (B,192,2) numpy\n",
    "\n",
    "def init_generate_samples_latents(model ,diffusion, checkpoint_path, NUM_TO_GENERATE , BATCH_SIZE):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            checkpoint_path:    the loading path for the weigths of the\n",
    "                                diffusion unet.\n",
    "            NUM_TO_GENERATE\n",
    "            BATCH_SIZE\n",
    "        output: \n",
    "            all_latents:    list of the latents used for sampling phase\n",
    "            all_shapes:     list of the shapes  generated from the latents\n",
    "    \"\"\"\n",
    "    # Load checkpoint\n",
    "    # checkpoint_path = rf\"src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\"\n",
    "    model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "    model.eval()\n",
    "    print(\"Loaded model weights from:\", checkpoint_path)\n",
    "\n",
    "    num_to_generate = NUM_TO_GENERATE\n",
    "    batch_size      = BATCH_SIZE\n",
    "\n",
    "    all_latent = []\n",
    "    all_shapes = []\n",
    "    with torch.no_grad():\n",
    "        done = 0\n",
    "        while done < num_to_generate:\n",
    "            cur = min(batch_size, num_to_generate - done)\n",
    "            \n",
    "            latent = torch.randn((cur,2,192)).to(device)\n",
    "            samples = diffusion.latent_sample(latent , is_ddim=True)\n",
    "            generated_real = inv_coords(samples)\n",
    "            \n",
    "            all_latent.append(latent.cpu().detach().numpy())\n",
    "            all_shapes.append(generated_real)\n",
    "            done += cur\n",
    "            print(f\"Generated {done}/{num_to_generate}\")\n",
    "\n",
    "    np.save(os.path.join(\"Database\" , \"DB_innerloop.npy\") , {\n",
    "        \"latents\": np.vstack(all_latent),\n",
    "        \"shapes\": np.vstack(all_shapes),\n",
    "    })\n",
    "\n",
    "def GEN_UA(diffusion,device ,num_cores, number_iter = 0,number_generations=100 , population_size = 1000 , from_DB_innerloop = True):\n",
    "    # n_iter = 2\n",
    "    print('calculating surrogate pareto ...')\n",
    "    if from_DB_innerloop:\n",
    "        DB_innerloop = np.load(os.path.join(\"Database\" , \"DB_innerloop.npy\"),allow_pickle=True).item()\n",
    "        full_samples = DB_innerloop[\"latents\"]  # (batch , 2,  192)\n",
    "        \n",
    "        print(full_samples.shape)\n",
    "    problem_uncertainty = BO_surrogate_uncertainty(diffusion = diffusion,device=device,num_cores=num_cores,n_iter=number_iter)\n",
    "    algorithm = NSGA2(pop_size=population_size)\n",
    "    res = minimize(problem_uncertainty,\n",
    "                algorithm,\n",
    "                ('n_gen', number_generations),\n",
    "                seed=1,\n",
    "                verbose=False,\n",
    "                X = full_samples.reshape(full_samples.shape[0], -1)  if from_DB_innerloop else None)\n",
    "    Paretoset_uncertainty = res.X\n",
    "    Out_surrogate_uncertainty = res.F\n",
    "    # sio.savemat('surrogate_pareto/ParetoSet_test.mat' , {'ParetoSet': np.array(Paretoset_uncertainty)})\n",
    "    # sio.savemat('surrogate_pareto/Out_surrogate_test.mat', {'Out_surrogate': np.array(Out_surrogate_uncertainty)})\n",
    "    print(f\"number of last generation sample is {len(Paretoset_uncertainty)}\")\n",
    "    np.save(os.path.join(\"Database\" , \"DB_NSGA.npy\"),{\n",
    "        'ParetoSet': np.array(Paretoset_uncertainty), # the latents\n",
    "        'Out_surrogate': np.array(Out_surrogate_uncertainty)\n",
    "    })\n",
    "\n",
    "    return problem_uncertainty\n",
    "\n",
    "def NSGA_latent_to_shape(model ,diffusion,num_cores, docker_mount_path, checkpoint_path, BATCH_SIZE=128):\n",
    "    # Load NSGA latent vectors\n",
    "    DB_NSGA = np.load(os.path.join(\"Database\", \"DB_NSGA.npy\"), allow_pickle=True).item()\n",
    "    NSGA_latent = DB_NSGA[\"ParetoSet\"].reshape(DB_NSGA[\"ParetoSet\"].shape[0], 2, -1)\n",
    "    NSGA_latent = torch.from_numpy(NSGA_latent).float()\n",
    "\n",
    "    # Load model weights\n",
    "    model.load_state_dict(torch.load(checkpoint_path, weights_only=True))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    print(\"Loaded model weights from:\", checkpoint_path)\n",
    "\n",
    "    # Create DataLoader for batching\n",
    "    dataset = TensorDataset(NSGA_latent)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_cores-1)\n",
    "\n",
    "    all_latent = []\n",
    "    all_shapes = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            latent_batch = batch[0].to(device)  # shape: (B, 2, 192)\n",
    "            samples = diffusion.latent_sample(latent_batch, is_ddim=True)\n",
    "            generated_real = inv_coords(samples)  # assumes inv_coords is defined globally\n",
    "\n",
    "            all_latent.append(latent_batch.cpu().numpy())\n",
    "            all_shapes.append(generated_real)\n",
    "\n",
    "            print(f\"Processed {(i+1)*BATCH_SIZE}/{len(NSGA_latent)} latents\")\n",
    "\n",
    "    # Save results\n",
    "    np.save(os.path.join(docker_mount_path, \"DB_CFD.npy\"), {\n",
    "        \"latents\": np.vstack(all_latent),\n",
    "        \"shapes\": np.vstack(all_shapes),\n",
    "        \"performance\": []\n",
    "    })\n",
    "    print(f\"Saved converted shapes to {os.path.join(docker_mount_path, 'DB_CFD.npy')}\")\n",
    "\n",
    "def CFD_simulation(docker_container_id):\n",
    "    # command = fr\"docker exec {docker_container_id} python3 /home/airfoil_UANA/performance_finding.py\"\n",
    "    command = (\n",
    "        f'docker exec {docker_container_id} bash -c \"'\n",
    "        f'source /opt/openfoam5/etc/bashrc && '\n",
    "        f'cd /home/airfoil_UANA && '\n",
    "        f'python3 innerloop_performance_finding.py\"'\n",
    "    )\n",
    "    os.system(command)\n",
    "\n",
    "def Tagging_phase(docker_mount_path, iteration  = 0):\n",
    "    \"\"\"\n",
    "        taggin the DB_CFD.npy and make it valid and invalid DB inn the Database directory\n",
    "        retraining the UA_surrogates and others.\n",
    "    \"\"\"\n",
    "    DB_CFD = np.load(os.path.join( docker_mount_path, \"DB_CFD.npy\"),allow_pickle=True).item()\n",
    "    performance = DB_CFD[\"performance\"]\n",
    "    latents = DB_CFD[\"latents\"]\n",
    "    shapes = DB_CFD[\"shapes\"]\n",
    "    # print(f\"{latents.shape=}\")\n",
    "    # print(f\"{shapes.shape=}\")\n",
    "    # print(f\"{performance.shape=}\")\n",
    "    valids = {\n",
    "        \"latents\" : [],\n",
    "        \"shapes\" : [],\n",
    "        \"performance\" : []\n",
    "    }\n",
    "    invalids = {\n",
    "        \"latents\" : [],\n",
    "        \"shapes\" : [],\n",
    "        \"performance\" : []\n",
    "    }\n",
    "    for i in range(len(latents)):\n",
    "        if performance[i,0] == -1000:\n",
    "            invalids[\"latents\"].append(latents[i])\n",
    "            invalids[\"shapes\"].append(shapes[i])\n",
    "            invalids[\"performance\"].append(performance[i])\n",
    "        else:\n",
    "            valids[\"latents\"].append(latents[i])\n",
    "            valids[\"shapes\"].append(shapes[i])\n",
    "            valids[\"performance\"].append(performance[i])\n",
    "\n",
    "    # sttackig the np arrays\n",
    "    if len(valids[\"latents\"]) > 0:\n",
    "        valids[\"latents\"] = np.vstack(valids[\"latents\"])\n",
    "        valids[\"shapes\"] = np.vstack(valids[\"shapes\"])\n",
    "        valids[\"performance\"] = np.vstack(valids[\"performance\"])\n",
    "    \n",
    "    if len(invalids[\"latents\"]) > 0:\n",
    "        invalids[\"latents\"] = np.vstack(invalids[\"latents\"])\n",
    "        invalids[\"shapes\"] = np.vstack(invalids[\"shapes\"])\n",
    "        invalids[\"performance\"] = np.vstack(invalids[\"performance\"])\n",
    "\n",
    "    \n",
    "    # print(f'{invalids[\"performance\"].shape=}')\n",
    "    # print(f'{valids[\"performance\"].shape=}')\n",
    "    \n",
    "    np.save(os.path.join(\"Database\" , f\"DB_valids_iter_{iteration}.npy\"),valids)\n",
    "    np.save(os.path.join(\"Database\" , f\"DB_invalids_iter_{iteration}.npy\"),invalids)\n",
    "\n",
    "    # Appending valids to the DB_innerloop\n",
    "    DB_innerloop = np.load(os.path.join(\"Database\",\"DB_innerloop.npy\"),allow_pickle=True).item()\n",
    "    if len(valids[\"latents\"]) > 0:\n",
    "        if valids[\"latents\"].dim() == 2:\n",
    "            appending_latents = np.expand_dims(valids[\"latents\"],axis = 0)\n",
    "            appending_shapes = np.expand_dims(valids[\"shapes\"],axis = 0)\n",
    "        \n",
    "        DB_innerloop[\"latents\"] = np.concatenate([DB_innerloop[\"latents\"] , appending_latents],axis = 0)\n",
    "        DB_innerloop[\"shapes\"] = np.concatenate([DB_innerloop[\"shapes\"] , appending_shapes],axis = 0)\n",
    "    \n",
    "    \n",
    "    print(f\"saving the valids and invalids in Database\")\n",
    "\n",
    "def Retraining_UA_modules(model , checkpoint_path ,num_cores, batch_size = 128,epoches = 20,patience=5,lr = 1e-6 , iteration = 0):\n",
    "    DB_valids = np.load(os.path.join(\"Database\", f\"DB_valids_iter_{iteration}.npy\"),allow_pickle=True).item()\n",
    "    if len(DB_valids[\"shapes\"]) ==  0:\n",
    "        print(\"No valid samples for this iteration passing the retraining ...\")\n",
    "        return \n",
    "    \n",
    "    airfoils = torch.from_numpy(DB_valids[\"shapes\"]).float()  # (batch, 192,2)\n",
    "    airfoils = airfoils.reshape(airfoils.shape[0],-1) # (batch, 384)\n",
    "    scores = torch.from_numpy(DB_valids[\"performance\"]).float()[:,:2] #(batch,2)  -> (cl,cd)\n",
    "    # Dataset\n",
    "    dataset = TensorDataset(airfoils, scores)\n",
    "    \n",
    "    # Loss + Optimizer\n",
    "    criterion = nn.L1Loss()   # L1 loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,num_workers=num_cores-1, shuffle=True)\n",
    "\n",
    "    losses = []\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epoches):\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epoches}\", leave=False)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for x, y in loop:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # forward\n",
    "            preds = model(x)[:,]\n",
    "            preds = torch.stack(preds,dim=0)\n",
    "            preds = torch.mean(preds,dim=0) # (batch , 2) -> cl , cl/cd\n",
    "            preds[:,1] = preds[:,0] / (preds[:,1] + 1e-10) # (batch , 2) -> cl , cd\n",
    "            loss = criterion(preds, y)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epoches} | Avg Loss: {total_loss/len(dataloader):.4f}\")    \n",
    "        \n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save best checkpoint\n",
    "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
    "            torch.save(losses , os.path.join(checkpoint_path , f\"losses_iter_{iteration}.pt\"))\n",
    "            torch.save(model.state_dict(), os.path.join(checkpoint_path , f\"UA_weigths_iter_{iteration}.pt\"))\n",
    "            print(f\"Saved new best model at epoch {epoch+1} with loss {best_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    # Retraining the Constraint Handler\n",
    "\n",
    "    print(f\"End retraining the modules\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n******************************\\n******************************\\nwarning:\\n    before running make sure to copy src/OpenFoam/Airfoil_simulation_1/OpenFOAM_0\\n    in that folder 200 times with new directories name as src/OpenFoam/Airfoil_simulation_1/OpenFOAM_i for the i'th core ussage\\n******************************\\n******************************\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The main Hyperparams\n",
    "\"\"\"\n",
    "iterations = 2\n",
    "num_cores = 2\n",
    "docker_mount_path = \"../../OpenFoam\"\n",
    "NUM_TO_GENERATE = 5\n",
    "BATCH_SIZE = 5\n",
    "docker_container_id = \"f897792b6b56\" \n",
    "Unet_checkpoint_path = rf\"../../../src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\"\n",
    "saving_path = rf\"src/optimization_loop/Inner_loop/Database/DB_innerloop.npy\"\n",
    "\n",
    "# Genetic  Algoorithm  Hyper params\n",
    "number_generations=1\n",
    "population_size=5\n",
    "from_DB_innerloop=True\n",
    "\n",
    "\"\"\"\n",
    "******************************\n",
    "******************************\n",
    "warning:\n",
    "    before running make sure to copy src/OpenFoam/Airfoil_simulation_1/OpenFOAM_0\n",
    "    in that folder 200 times with new directories name as src/OpenFoam/Airfoil_simulation_1/OpenFOAM_i for the i'th core ussage\n",
    "******************************\n",
    "******************************\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Procedure\n",
    "1.  creating and generating the `DB_innerloop` for the shapes and latents holding (just these two) ->  OK\n",
    "2.  making the `UA_information` data for the whole `DB_innerloop` and save it in `DB_UA` for the NSGA algorithm ->  OK\n",
    "3.  using the `NSGA` algorithm and  creating the `DB_NSGA` -> OK\n",
    "4.  give the results of the `DB_NSGA` to the Openfoam and  create `DB_OpenFoam`\n",
    "5.  evaluate and tag the `DB_Openfoam` and append the correct data to the `DB_Valid` and `DB_Invalid`\n",
    "6.  retrain the `UA_surrogate_Model` with the valid datas\n",
    "7.  redo  all above part for `N` Epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stage 1:\n",
    "# init_generate_samples_latents(Unet_model , \n",
    "#                             diffusion , \n",
    "#                             NUM_TO_GENERATE=NUM_TO_GENERATE,\n",
    "#                             BATCH_SIZE=BATCH_SIZE,\n",
    "#                             checkpoint_path=Unet_checkpoint_path\n",
    "#                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating surrogate pareto ...\n",
      "(5, 2, 192)\n",
      "\n",
      "Compiled modules for significant speedup can not be used!\n",
      "https://pymoo.org/installation.html#installation\n",
      "\n",
      "To disable this warning:\n",
      "from pymoo.config import Config\n",
      "Config.warnings['not_compiled'] = False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:36<00:00, 27.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 128 / 5 latents\n",
      "number of last generation sample is 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bardiya/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from utils import  BO_surrogate_uncertainty\n",
    "\n",
    "# Stage 2:\n",
    "NSGA_BO_surrogate_modules = GEN_UA(diffusion=diffusion, \n",
    "                                   device=device,\n",
    "                                   num_cores = num_cores, \n",
    "                                   number_generations=1, \n",
    "                                   population_size=5,\n",
    "                                   from_DB_innerloop=True)\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model weights from: ../../../src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:35<00:00, 28.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/4 latents\n",
      "Saved converted shapes to ../../OpenFoam/DB_CFD.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bardiya/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Stage 3:\n",
    "NSGA_latent_to_shape(Unet_model , \n",
    "                diffusion , \n",
    "                num_cores,\n",
    "                BATCH_SIZE=BATCH_SIZE,\n",
    "                checkpoint_path=Unet_checkpoint_path,\n",
    "                docker_mount_path=docker_mount_path\n",
    "                )\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4 (first  start it):\n",
    "CFD_simulation(docker_container_id=docker_container_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving the valids and invalids in Database\n"
     ]
    }
   ],
   "source": [
    "# Stage 5:\n",
    "Tagging_phase(docker_mount_path , iteration=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid samples for this iteration passing the retraining ...\n"
     ]
    }
   ],
   "source": [
    "# Stage 6: (retraining the UA_surrogate models [and possibly the constraint handler to])\n",
    "checkpoint_path = \"Retraining_modules\"\n",
    "Retraining_UA_modules(iteration=0,\n",
    "                      num_cores=num_cores,\n",
    "                      model=NSGA_BO_surrogate_modules.UA_surrogate_model,\n",
    "                      checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "Stage 1\n",
      "Loaded model weights from: ../../../src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:38<00:00, 25.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 5/5\n",
      "Stage 2\n",
      "calculating surrogate pareto ...\n",
      "(5, 2, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [00:40<00:00, 24.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 128 / 5 latents\n",
      "number of last generation sample is 1\n",
      "Stage 3\n",
      "Loaded model weights from: ../../../src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:15<00:00, 65.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/1 latents\n",
      "Saved converted shapes to ../../OpenFoam/DB_CFD.npy\n",
      "Stage 4\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Error   : Unable to recover the edge 1946 (18/435) on GEdge 1000 (on GFace 1026)\n",
      "Warning : Surface 1 consists of no elements\n",
      "Warning : Volume 1 consists of no elements\n",
      "Error   : ------------------------------\n",
      "Error   : Mesh generation error summary\n",
      "Error   :    49 warnings\n",
      "Error   :    10 errors\n",
      "Error   : Check the full log for details\n",
      "Error   : ------------------------------\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/airfoil_UANA/Airfoil_simulation_1/ShapeToPerformance.py\", line 211, in calculate_cd_cl_res\n",
      "    if runSim(fsX, fsY) != 0:\n",
      "  File \"/home/airfoil_UANA/Airfoil_simulation_1/ShapeToPerformance.py\", line 89, in runSim\n",
      "    with open(\"U_template\", \"rt\") as inFile:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'U_template'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core 0:\n",
      "sample 0:\n",
      "\tUsing len 40.000 angle +0.000 \n",
      "\tResulting freestream vel x,y: 40.0,-0.0\n",
      "error during mesh creation!\n",
      "\tmesh generation failed, aborting\n",
      "-1000\n",
      "1000\n",
      "/home/airfoil_UANA\n",
      "/home/airfoil_UANA\n",
      "0.txt\n",
      "[0]\n",
      "[[-1000.  1000.     0.]]\n",
      "CFD simulation done!!!\n",
      "Stage 5\n",
      "saving the valids and invalids in Database\n",
      "Stage 6\n",
      "No valid samples for this iteration passing the retraining ...\n",
      "iteration: 1\n",
      "Stage 2\n",
      "calculating surrogate pareto ...\n",
      "(5, 2, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:42<00:00, 23.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 128 / 5 latents\n",
      "number of last generation sample is 1\n",
      "Stage 3\n",
      "Loaded model weights from: ../../../src/diffusion_notebooks/DIffusion_model_weigths_and_datas/dpp_0.1_autonorm_true_125_from_base_ddpm/model_epoch_124.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 1000/1000 [00:16<00:00, 62.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/1 latents\n",
      "Saved converted shapes to ../../OpenFoam/DB_CFD.npy\n",
      "Stage 4\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Warning : :-( There are 2 intersections in the 1D mesh (curves 1000 1000)\n",
      "Warning : 8-| Gmsh splits those edges and tries again\n",
      "Warning : Surface 1 consists of no elements\n",
      "Error   : Could not find extruded vertex (0.9669959659711346, 0.05854465817589253, 1) in surface 1026\n",
      "Error   : Unable to recover the edge 1946 (18/435) on GEdge 1000 (on GFace 1026)\n",
      "Warning : Surface 1 consists of no elements\n",
      "Warning : Volume 1 consists of no elements\n",
      "Error   : ------------------------------\n",
      "Error   : Mesh generation error summary\n",
      "Error   :    49 warnings\n",
      "Error   :    10 errors\n",
      "Error   : Check the full log for details\n",
      "Error   : ------------------------------\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/airfoil_UANA/Airfoil_simulation_1/ShapeToPerformance.py\", line 211, in calculate_cd_cl_res\n",
      "    if runSim(fsX, fsY) != 0:\n",
      "  File \"/home/airfoil_UANA/Airfoil_simulation_1/ShapeToPerformance.py\", line 89, in runSim\n",
      "    with open(\"U_template\", \"rt\") as inFile:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'U_template'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core 0:\n",
      "sample 0:\n",
      "\tUsing len 40.000 angle +0.000 \n",
      "\tResulting freestream vel x,y: 40.0,-0.0\n",
      "error during mesh creation!\n",
      "\tmesh generation failed, aborting\n",
      "-1000\n",
      "1000\n",
      "/home/airfoil_UANA\n",
      "/home/airfoil_UANA\n",
      "0.txt\n",
      "[0]\n",
      "[[-1000.  1000.     0.]]\n",
      "CFD simulation done!!!\n",
      "Stage 5\n",
      "saving the valids and invalids in Database\n",
      "Stage 6\n",
      "No valid samples for this iteration passing the retraining ...\n",
      "End  Inner loop procedure\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "from pymoo.algorithms.moo.nsga2 import NSGA2\n",
    "from pymoo.optimize import minimize\n",
    "from utils import  BO_surrogate_uncertainty\n",
    "\n",
    "for iter in range(iterations):\n",
    "    print(f\"iteration: {iter}\")\n",
    "    # Stage 1:\n",
    "    print(\"Stage 1\")\n",
    "    NSGA_BO_surrogate_modules = GEN_UA(diffusion=diffusion, \n",
    "                                   device=device,\n",
    "                                   num_cores = num_cores, \n",
    "                                   number_generations=number_generations, \n",
    "                                   population_size=population_size,\n",
    "                                   from_DB_innerloop=True)\n",
    "\n",
    "    # Stage 2:\n",
    "    print(\"Stage 2\")\n",
    "    NSGA_latent_to_shape(Unet_model , \n",
    "                diffusion , \n",
    "                num_cores,\n",
    "                BATCH_SIZE=BATCH_SIZE,\n",
    "                checkpoint_path=Unet_checkpoint_path,\n",
    "                docker_mount_path=docker_mount_path\n",
    "                )\n",
    "\n",
    "    # Stage 3 (first  start it):\n",
    "    print(\"Stage 3\")\n",
    "    CFD_simulation(docker_container_id=docker_container_id)\n",
    "\n",
    "    # Stage 4:\n",
    "    print(\"Stage 4\")\n",
    "    Tagging_phase(docker_mount_path , iteration=iter)\n",
    "\n",
    "    # Stage 5: (retraining the UA_surrogate models [and possibly the constraint handler to])\n",
    "    print(\"Stage 5\")\n",
    "    checkpoint_path = \"Retraining_modules\"\n",
    "    Retraining_UA_modules(iteration=iter,\n",
    "                        num_cores=num_cores,\n",
    "                        model=NSGA_BO_surrogate_modules.UA_surrogate_model,\n",
    "                        checkpoint_path=checkpoint_path)\n",
    "print(\"End  Inner loop procedure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
