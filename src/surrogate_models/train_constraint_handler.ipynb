{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os,sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "sys.path.append(\"../surrogate_models\")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "activation_function_list = [torch.tanh, nn.ReLU(), nn.CELU(), nn.LeakyReLU(), nn.ELU(), nn.Hardswish(),torch.tanh, nn.ReLU(), nn.CELU(), nn.LeakyReLU(), torch.tanh]\n",
    "\n",
    "class MultiLayerPerceptron_forward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, net_n):\n",
    "        super(MultiLayerPerceptron_forward, self).__init__()\n",
    "        #################################################################################\n",
    "        # Initialize the modules required to implement the mlp with given layer   #\n",
    "        # configuration. input_size --> hidden_layers[0] --> hidden_layers[1] .... -->  #\n",
    "        # hidden_layers[-1] --> num_classes                                             #\n",
    "        #################################################################################\n",
    "        layers = []\n",
    "        layers.append(nn.Linear((input_size), (hidden_layers[0])))\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            layers.append(nn.Linear((hidden_layers[i]), (hidden_layers[i+1])))\n",
    "\n",
    "        layers.append(nn.Linear((hidden_layers[len(hidden_layers)-1]), (num_classes)))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.net_n = net_n\n",
    "        self.hidden_layers = hidden_layers\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        m = activation_function_list[self.net_n]\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            x = self.layers[i](x)\n",
    "            x = m(x)\n",
    "        x = (self.layers[len(self.hidden_layers)](x))\n",
    "        out=x\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron_forward_classifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes):\n",
    "        super(MultiLayerPerceptron_forward_classifier, self).__init__()\n",
    "        #################################################################################\n",
    "        # Initialize the modules required to implement the mlp with given layer   #\n",
    "        # configuration. input_size --> hidden_layers[0] --> hidden_layers[1] .... -->  #\n",
    "        # hidden_layers[-1] --> num_classes                                             #\n",
    "        #################################################################################\n",
    "        layers = []\n",
    "        layers.append(nn.Linear((input_size), (hidden_layers[0])))\n",
    "        # layers.append(nn.Linear((hidden_layers[0]), (hidden_layers[1])))\n",
    "        # layers.append(nn.Linear((hidden_layers[1]), (hidden_layers[2])))\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            layers.append(nn.Linear((hidden_layers[i]), (hidden_layers[i+1])))\n",
    "\n",
    "        layers.append(nn.Linear((hidden_layers[len(hidden_layers)-1]), (num_classes)))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.hidden_size = hidden_layers\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "\n",
    "        # x = F.relu(self.layers[0](x))\n",
    "        # x = F.relu(self.layers[1](x))\n",
    "        # x = F.relu(self.layers[2](x))\n",
    "        for i in range(len(self.hidden_size)):\n",
    "            x = F.relu(self.layers[i](x))\n",
    "        x = (self.layers[len(self.hidden_size)](x))\n",
    "        out = x\n",
    "        # out = F.sigmoid(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "#===================================================    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# List of activation functions\n",
    "activation_function_list = [\n",
    "    torch.tanh, nn.ReLU(), nn.CELU(), nn.LeakyReLU(), nn.ELU(),\n",
    "    nn.Hardswish(), torch.tanh, nn.ReLU(), nn.CELU(), nn.LeakyReLU(), torch.tanh\n",
    "]\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Learnable linear projections\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key   = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scale = embed_dim ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, embed_dim)\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Attention scores: (batch, seq_len, seq_len)\n",
    "        scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum: (batch, seq_len, embed_dim)\n",
    "        attn_output = torch.bmm(attn_weights, V)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class MultiLayerPerceptronWithCustomAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, net_n):\n",
    "        super(MultiLayerPerceptronWithCustomAttention, self).__init__()\n",
    "\n",
    "        # MLP Layers\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_layers[0]))\n",
    "        for i in range(len(hidden_layers) - 1):\n",
    "            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n",
    "        self.mlp_layers = nn.ModuleList(layers)\n",
    "        self.activation = activation_function_list[net_n]\n",
    "        \n",
    "        self.attention = SelfAttention(embed_dim=hidden_layers[-1])\n",
    "        self.final_linear = nn.Linear(hidden_layers[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # MLP forward\n",
    "        for layer in self.mlp_layers:\n",
    "            x = self.activation(layer(x))\n",
    "\n",
    "        # Reshape for attention: treat features as a sequence of length 1\n",
    "        x = x.unsqueeze(1)  # (batch, 1, embed_dim)\n",
    "\n",
    "        # Apply attention\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # Flatten back to (batch, embed_dim)\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        # Final output\n",
    "        out = self.final_linear(x)\n",
    "        return out\n",
    "\n",
    "#====================================================\n",
    "\n",
    "class ResidualBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 1D residual block with optional downsampling.\n",
    "    The first conv can do stride=2 if downsample=True, the second conv uses stride=1.\n",
    "    We'll use kernel=5, pad=2 so that stride=1 doesn't reduce length at all.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "\n",
    "        # If we want to downsample, we set stride=2 for the first conv and the shortcut\n",
    "        stride_first = 2 if downsample else 1\n",
    "        kernel_size = 5\n",
    "        pad = 2\n",
    "\n",
    "        # 1) First conv\n",
    "        self.bn1 = nn.BatchNorm1d(in_channels, momentum=0.9)\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride_first,\n",
    "                               padding=pad)\n",
    "        # 2) Second conv always stride=1\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels, momentum=0.9)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=1,\n",
    "                               padding=pad)\n",
    "\n",
    "        # Shortcut (for downsample or channel mismatch)\n",
    "        if downsample or (in_channels != out_channels):\n",
    "            self.shortcut_conv = nn.Conv1d(in_channels, out_channels,\n",
    "                                           kernel_size=1,\n",
    "                                           stride=stride_first,\n",
    "                                           padding=0)\n",
    "        else:\n",
    "            self.shortcut_conv = nn.Identity()\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        if debug:\n",
    "            print(f\"  [ResidualBlock] Input shape: {x.shape}\")\n",
    "        identity = self.shortcut_conv(x)     # Possibly stride=2 if downsample is True\n",
    "\n",
    "        # First conv\n",
    "        out = self.bn1(x)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.conv1(out)\n",
    "        if debug:\n",
    "            print(f\"    After conv1 shape: {out.shape}\")\n",
    "\n",
    "        # Second conv\n",
    "        out = self.bn2(out)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.conv2(out)\n",
    "        if debug:\n",
    "            print(f\"    After conv2 shape: {out.shape}\")\n",
    "\n",
    "        # Add skip\n",
    "        out = out + identity\n",
    "        if debug:\n",
    "            print(f\"  [ResidualBlock] Output shape: {out.shape}\\n\")\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SurrogateModel1D(nn.Module):\n",
    "    \"\"\"\n",
    "    A 1D ResNet-like approach for (batch, 2, n_points).\n",
    "    We'll do four \"groups\" of blocks, each of which can downsample the length dimension in the first block.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_points=192, in_channels=2, depth=16, residual_list=[2,2,2,2]):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.residual_list = residual_list\n",
    "\n",
    "        # -- Initial Conv: kernel=5, pad=2, stride=1 => length stays 192\n",
    "        self.conv_initial = nn.Conv1d(in_channels, depth, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn_initial = nn.BatchNorm1d(depth, momentum=0.9)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        # Group0: no downsample in first block\n",
    "        self.blocks0 = nn.ModuleList([\n",
    "            ResidualBlock1D(depth, depth, downsample=False)\n",
    "            for _ in range(residual_list[0])\n",
    "        ])\n",
    "        # Group1: first block downsample => depth-> depth*2\n",
    "        self.resblock1_0 = ResidualBlock1D(depth, depth*2, downsample=True)\n",
    "        self.blocks1 = nn.ModuleList([\n",
    "            ResidualBlock1D(depth*2, depth*2, downsample=False)\n",
    "            for _ in range(residual_list[1]-1)\n",
    "        ])\n",
    "        # Group2\n",
    "        self.resblock2_0 = ResidualBlock1D(depth*2, depth*4, downsample=True)\n",
    "        self.blocks2 = nn.ModuleList([\n",
    "            ResidualBlock1D(depth*4, depth*4, downsample=False)\n",
    "            for _ in range(residual_list[2]-1)\n",
    "        ])\n",
    "        # Group3\n",
    "        self.resblock3_0 = ResidualBlock1D(depth*4, depth*8, downsample=True)\n",
    "        self.blocks3 = nn.ModuleList([\n",
    "            ResidualBlock1D(depth*8, depth*8, downsample=False)\n",
    "            for _ in range(residual_list[3]-1)\n",
    "        ])\n",
    "\n",
    "        # Final BN + global avg pool + FC(128) + BN + LReLU + FC(2) + Sigmoid\n",
    "        self.bn_final = nn.BatchNorm1d(depth*8, momentum=0.9)\n",
    "        self.fc1 = nn.Linear(depth*8, 128)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128, momentum=0.9)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        if debug:\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "        out = self.conv_initial(x)\n",
    "        out = self.bn_initial(out)\n",
    "        out = self.lrelu(out)\n",
    "        if debug:\n",
    "            print(f\"After initial conv shape: {out.shape}\")\n",
    "\n",
    "        # group0\n",
    "        for block in self.blocks0:\n",
    "            out = block(out, debug=debug)\n",
    "        if debug:\n",
    "            print(f\"After group0 shape: {out.shape}\")\n",
    "\n",
    "        # group1\n",
    "        out = self.resblock1_0(out, debug=debug)\n",
    "        for block in self.blocks1:\n",
    "            out = block(out, debug=debug)\n",
    "        if debug:\n",
    "            print(f\"After group1 shape: {out.shape}\")\n",
    "\n",
    "        # group2\n",
    "        out = self.resblock2_0(out, debug=debug)\n",
    "        for block in self.blocks2:\n",
    "            out = block(out, debug=debug)\n",
    "        if debug:\n",
    "            print(f\"After group2 shape: {out.shape}\")\n",
    "\n",
    "        # group3\n",
    "        out = self.resblock3_0(out, debug=debug)\n",
    "        for block in self.blocks3:\n",
    "            out = block(out, debug=debug)\n",
    "        if debug:\n",
    "            print(f\"After group3 shape: {out.shape}\")\n",
    "\n",
    "        out = self.bn_final(out)\n",
    "        out = self.lrelu(out)\n",
    "        if debug:\n",
    "            print(f\"After bn_final + lrelu shape: {out.shape}\")\n",
    "\n",
    "        # global avg pool => (batch, depth*8, 1)\n",
    "        out = F.adaptive_avg_pool1d(out, 1)\n",
    "        if debug:\n",
    "            print(f\"After global avg pool shape: {out.shape}\")\n",
    "        out = out.view(out.size(0), -1)  # => (batch, depth*8)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn_fc1(out)\n",
    "        out = self.lrelu(out)\n",
    "        if debug:\n",
    "            print(f\"After fc1 shape: {out.shape}\")\n",
    "\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        if debug:\n",
    "            print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "#====================================================\n",
    "\n",
    "class Hybrid_surrogate_MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 hidden_layers_cl_model ,\n",
    "                 hidden_layers_cd_model , \n",
    "                 net_n_cl=3 , \n",
    "                 net_n_cd=3, \n",
    "                 path_cl_model = None, \n",
    "                 path_cd_model  =  None):\n",
    "        super(Hybrid_surrogate_MLP, self).__init__()\n",
    "        self.cl_forward_mlp = MultiLayerPerceptron_forward(input_size , hidden_layers_cl_model ,   num_classes=1  , net_n=net_n_cl)\n",
    "        self.cd_forward_mlp = MultiLayerPerceptron_forward(input_size , hidden_layers_cd_model ,   num_classes=1  , net_n=net_n_cd)\n",
    "        if path_cl_model:\n",
    "            self.cl_forward_mlp.load_state_dict(torch.load(path_cl_model,map_location=\"cpu\"))\n",
    "        if path_cd_model:\n",
    "            self.cd_forward_mlp.load_state_dict(torch.load(path_cd_model,map_location=\"cpu\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        # Implement the forward pass computations                                 #\n",
    "        #################################################################################\n",
    "        cl = self.cl_forward_mlp(x)\n",
    "        cd = self.cd_forward_mlp(x)\n",
    "        return torch.stack([cl,cd],dim=1).squeeze(-1)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['latents', 'shapes', 'performances'])\n",
      "[  7.  73. 182. ... 162.  93.  42.]\n",
      "xs_train.shape=(10000, 384)\n",
      "ys_train.shape=(9991, 3)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m y_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(ys_train)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print(ys_train[:3])\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# sys.exit()\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.9\u001b[39m), \u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.9\u001b[39m)]\n\u001b[1;32m     50\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(dataset, lengths)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataset.py:205\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    206\u001b[0m         tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors\n\u001b[1;32m    207\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "import scipy.io as sio\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as  plt\n",
    "\n",
    "\n",
    "input_size = 192 * 2\n",
    "# hidden_size = [150, 200, 200 , 150]\n",
    "hidden_size = [200, 300, 300 , 200]\n",
    "# hidden_size = [128, 64 , 32]\n",
    "num_classes = 1\n",
    "num_epochs = 250\n",
    "learning_rate = 9e-4\n",
    "# learning_rate = 1e-6\n",
    "# learning_rate = 1e-7\n",
    "learning_rate_decay = 0.999\n",
    "reg = 0.001\n",
    "batch_size = 128\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "num_workers = 2\n",
    "patience = 100\n",
    "\n",
    "# xs_train = np.load(rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/data/raw/xs_train.npy\")\n",
    "# ys_train = np.load(rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/data/raw/ys_train.npy\")\n",
    "\n",
    "db = np.load(rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/surrogate_models/run_results.npy\",allow_pickle=True).item()\n",
    "\n",
    "print(db.keys())\n",
    "# sys.exit()\n",
    "xs_train = db[\"shapes\"]\n",
    "ys_train = db[\"performances\"]\n",
    "xs_train = xs_train.reshape(xs_train.shape[0],-1)\n",
    "print(ys_train[:,-1])\n",
    "print(f\"{xs_train.shape=}\")\n",
    "print(f\"{ys_train.shape=}\")\n",
    "x_train_tensor = torch.from_numpy(xs_train).float()\n",
    "y_train_tensor = torch.from_numpy(ys_train).float()\n",
    "# print(ys_train[:3])\n",
    "# sys.exit()\n",
    "\n",
    "dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "lengths = [int(len(dataset)*0.9), len(dataset)-int(len(dataset)*0.9)]\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, lengths)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size , \n",
    "                                           num_workers=  num_workers,\n",
    "                                           drop_last= True , \n",
    "                                           shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=batch_size , \n",
    "                                         num_workers = num_workers)\n",
    "\n",
    "\n",
    "\n",
    "# model_mlp = MultiLayerPerceptronWithCustomAttention(input_size, hidden_size, num_classes,3).to(device)\n",
    "model_mlp = MultiLayerPerceptron_forward_classifier(input_size, hidden_size, num_classes,3).to(device)\n",
    "print(count_parameters(model_mlp))\n",
    "\n",
    "# model_mlp.apply(weights_init)\n",
    "# model_mlp.load_state_dict(torch.load(\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/surrogate_models/mlp_best_model.pt\",map_location=\"cpu\"))\n",
    "model_mlp.to(device)\n",
    "\n",
    "\n",
    "# criterion = criterion_MAE\n",
    "# criterion_train= sum_mse\n",
    "# criterion_val   = nn.MSELoss()  # Standard mean MSE\n",
    "criterion_train= nn.BCEWithLogitsLoss()\n",
    "criterion_val   = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model_mlp.parameters(), lr=learning_rate)\n",
    "\n",
    "########################################\n",
    "# 5) Training loop with early stopping\n",
    "########################################\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epochs_no_improve = 0\n",
    "lr_current = learning_rate\n",
    "\n",
    "train_losses = []\n",
    "test_losses  = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # ---- Training ----\n",
    "    model_mlp.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "        features, temp_labels = features.to(device), labels.to(device)\n",
    "        labels= torch.zeros((temp_labels.shape[0],1))\n",
    "        for i in range(temp_labels.shape[0]):\n",
    "            if temp_labels[i,0] == -1000:\n",
    "                labels[i,0] = 0\n",
    "            else:\n",
    "                labels[i,0] = 1\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_mlp(features)  # shape (batch, 1)\n",
    "    \n",
    "        print(f\"{outputs.shape=}\")\n",
    "        print(f\"{labels.shape=}\")\n",
    "        sys.exit()\n",
    "        loss = criterion_train(outputs.squeeze(dim=1), labels[:,0])   #cl\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * features.size(0)\n",
    "\n",
    "        # if (batch_idx + 1) % 100 == 0:\n",
    "        #     print(f\"Epoch [{epoch}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}],\"\n",
    "        #           f\" Train Loss (batch): {loss.item():.4f}\")\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model_mlp.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            preds = model_mlp(features)\n",
    "            # print(f\"{preds.shape=}\")\n",
    "            # print(f\"{labels[:,1].shape=}\")\n",
    "            # sys.exit()\n",
    "            # test_loss += criterion_val(preds.squeeze(dim=1), labels[:,1]).item() * features.size(0)\n",
    "            test_loss += criterion_val(preds.squeeze(dim=1), labels[:,0]).item() * features.size(0)\n",
    "    epoch_test_loss = test_loss / len(test_loader.dataset)\n",
    "    test_losses.append(epoch_test_loss)\n",
    "\n",
    "    # Print log\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {epoch_train_loss:.6f}, Test Loss: {epoch_test_loss:.6f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_test_loss < best_loss:\n",
    "        best_loss = epoch_test_loss\n",
    "        best_epoch = epoch\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model_mlp.state_dict(), \"mlp_best_model.pt\")\n",
    "        print(f\"New best model at epoch {epoch} with test loss {best_loss:.6f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"No improvement for {patience} epochs. Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "    # Update learning rate\n",
    "    lr_current *= learning_rate_decay\n",
    "    update_lr(optimizer, lr_current)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {elapsed:.2f} seconds. Best epoch: {best_epoch} with test loss {best_loss:.6f}\")\n",
    "\n",
    "# Save final model weights\n",
    "# torch.save(model_mlp.state_dict(), \"mlp_final_model.pt\")\n",
    "# torch.save(model_mlp.state_dict(), \"mlp_cl_model.pt\")\n",
    "torch.save(model_mlp.state_dict(), \"mlp_cd_model.pt\")\n",
    "print(\"Saved mlp_final_model.pt\")\n",
    "\n",
    "\n",
    "############################################################\n",
    "# 6) Save and plot train/test losses in separate subplots\n",
    "############################################################\n",
    "train_losses = np.array(train_losses)\n",
    "test_losses  = np.array(test_losses)\n",
    "\n",
    "# Save them as .npy\n",
    "np.save(\"train_losses.npy\", train_losses)\n",
    "np.save(\"test_losses.npy\",  test_losses)\n",
    "\n",
    "# Two subplots: one for Train Loss, one for Test Loss\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "axs[0].set_title(\"Train Loss vs Epoch\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].grid(True, linestyle=\"--\", alpha=0.7)\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(test_losses, label=\"Test Loss\", color=\"orange\")\n",
    "axs[1].set_title(\"Test Loss vs Epoch\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].grid(True, linestyle=\"--\", alpha=0.7)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 9991)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "db = np.load(rf\"/home/bardiya/projects/diffusion_air_manifolding/codes/creative-generativeai-diffusion/src/surrogate_models/run_results.npy\",allow_pickle=True).item()\n",
    "xs_train = db[\"shapes\"]\n",
    "ys_train = db[\"performances\"]\n",
    "xs_train = xs_train.reshape(xs_train.shape[0],-1)\n",
    "len(np.unique(ys_train[:,2])), len(ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
